"use strict";(globalThis.webpackChunkdocusaurus_book=globalThis.webpackChunkdocusaurus_book||[]).push([[168],{31:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapters/vision-language-action-robotics","title":"Chapter 7: Vision-Language-Action Robotics","description":"Welcome to the cutting-edge of robotics! In this chapter, we\'ll explore the exciting field of Vision-Language-Action (VLA) robotics. VLA models are a type of AI that can understand natural language, perceive the world through vision, and take actions in the physical world.","source":"@site/docs/chapters/07-vision-language-action-robotics.mdx","sourceDirName":"chapters","slug":"/chapters/vision-language-action-robotics","permalink":"/docs/chapters/vision-language-action-robotics","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"sidebar_position":11},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 6: NVIDIA Isaac Sim & Isaac ROS","permalink":"/docs/chapters/nvidia-isaac-sim-and-isaac-ros"},"next":{"title":"Chapter 8: Conversational Robotics (GPT + Whisper)","permalink":"/docs/chapters/conversational-robotics"}}');var t=o(4848),a=o(8453);const s={sidebar_position:11},r="Chapter 7: Vision-Language-Action Robotics",l={},c=[{value:"Understanding Vision-Language-Action (VLA) Models",id:"understanding-vision-language-action-vla-models",level:2},{value:"Training and Deploying VLA Models on Robots",id:"training-and-deploying-vla-models-on-robots",level:2},{value:"Building a Simple &quot;Pick and Place&quot; Robot with VLA",id:"building-a-simple-pick-and-place-robot-with-vla",level:2},{value:"Student Activity: Command Your Robot",id:"student-activity-command-your-robot",level:2},{value:"Mini Quiz",id:"mini-quiz",level:2}];function d(e){const n={em:"em",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-7-vision-language-action-robotics",children:"Chapter 7: Vision-Language-Action Robotics"})}),"\n",(0,t.jsx)(n.p,{children:"Welcome to the cutting-edge of robotics! In this chapter, we'll explore the exciting field of Vision-Language-Action (VLA) robotics. VLA models are a type of AI that can understand natural language, perceive the world through vision, and take actions in the physical world."}),"\n",(0,t.jsx)(n.h2,{id:"understanding-vision-language-action-vla-models",children:"Understanding Vision-Language-Action (VLA) Models"}),"\n",(0,t.jsx)(n.p,{children:"VLA models are at the forefront of AI research. They are typically large, deep learning models that are trained on massive datasets of images, text, and robot actions. This allows them to learn a rich understanding of the world and to generalize to new tasks and environments."}),"\n",(0,t.jsx)(n.p,{children:"Some of the key architectures used in VLA models include:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Transformers:"})," The same architecture that powers large language models like GPT-3."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision Transformers (ViTs):"})," A variation of the transformer architecture that is designed to work with images."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multimodal Fusion:"})," Techniques for combining information from different modalities, such as vision and language."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"training-and-deploying-vla-models-on-robots",children:"Training and Deploying VLA Models on Robots"}),"\n",(0,t.jsx)(n.p,{children:"Training a VLA model from scratch can be a massive undertaking, requiring huge amounts of data and computational resources. Fortunately, there are a number of pre-trained VLA models available that you can fine-tune for your own specific tasks."}),"\n",(0,t.jsx)(n.p,{children:"We'll explore the process of:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fine-tuning a pre-trained VLA model:"})," How to adapt a pre-trained model to your own specific needs."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deploying a VLA model on a robot:"})," The challenges of running a large deep learning model on a resource-constrained robot."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-world considerations:"})," The importance of safety, reliability, and ethical considerations when deploying VLA models in the real world."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"building-a-simple-pick-and-place-robot-with-vla",children:'Building a Simple "Pick and Place" Robot with VLA'}),"\n",(0,t.jsx)(n.p,{children:'The best way to understand VLA models is to see them in action. We\'ll walk you through the process of building a simple "pick and place" robot that uses a VLA model to understand natural language commands. For example, you\'ll be able to tell the robot "pick up the red block" and it will be able to understand your command and execute the task.'}),"\n",(0,t.jsx)(n.h2,{id:"student-activity-command-your-robot",children:"Student Activity: Command Your Robot"}),"\n",(0,t.jsx)(n.p,{children:'Think of five different commands you could give to a "pick and place" robot. Try to make your commands increasingly complex. For example, you could start with a simple command like "pick up the blue block" and then move on to more complex commands like "stack the green block on top of the yellow block".'}),"\n",(0,t.jsx)(n.h2,{id:"mini-quiz",children:"Mini Quiz"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"What does VLA stand for?"}),"\r\na) Vision-Language-Architecture\r\nb) Vision-Language-Action\r\nc) Vision-Language-Actuator"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Which architecture is commonly used in VLA models?"}),"\r\na) Recurrent Neural Networks (RNNs)\r\nb) Convolutional Neural Networks (CNNs)\r\nc) Transformers"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"True or False: Training a VLA model from scratch is a simple and inexpensive process."})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"Answers: 1(b), 2(c), 3(False)"})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>r});var i=o(6540);const t={},a=i.createContext(t);function s(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);